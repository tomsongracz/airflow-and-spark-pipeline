# Airflow and Spark ETL Pipeline on GCS Lakehouse

End-to-end pipeline danych z orkiestracjÄ… w **Apache Airflow**, przetwarzaniem w **Apache Spark** i architekturÄ… **Data Lakehouse** w **Google Cloud Platform (GCS + BigQuery)**. 
Projekt demonstruje peÅ‚ny cykl ETL na danych Netflix z Kaggle â€” od pobrania, przez czyszczenie i deduplikacjÄ™ w **Spark**, po zaÅ‚adowanie danych do **BigQuery**.

---

## ğŸ›  Stack technologiczny

| **Komponent**       | **Technologia** | **Rola** |
|----------------------|-----------------|-----------|
| **Orkiestracja**    | Apache Airflow 2.8.1 (Docker, CeleryExecutor) | DAG-i, scheduling, triggerowanie, monitoring (UI: `localhost:8080`) |
| **Przetwarzanie**   | Apache Spark 3.5.3 (PySpark, Python 3.10.12, Docker cluster: master + worker) | ETL: czyszczenie, deduplikacja, zapis Parquet (UI: `localhost:8081`) |
| **Data Lake**       | Google Cloud Storage (GCS) | Warstwy Raw / Processed / Temp (buckety: `-raw`, `-processed`, `-temp`) |
| **Hurtownia danych**| Google BigQuery | Åadowanie Parquet, schemat tabeli, analityka |
| **CI/CD**           | GitHub Actions | Testy Python, linting (Black/Flake8), build Docker |
| **Testy**           | pytest + Makefile (Python 3.10.12) | Testy DAG-Ã³w (import, zaleÅ¼noÅ›ci) i Spark ETL (czyszczenie / duplikaty) |
| **Integracje**      | Kaggle API, GCS Connector JAR, Java 17 (dla SparkSubmitOperator) | Pobieranie danych, uwierzytelnianie GCP |

> Kod sformatowany za pomocÄ… **Black** + **Flake8** (ignorowanie `E501` dla dÅ‚ugich linii).

---

## ğŸš€ Opis projektu

Codzienny pipeline orkiestrowany przez **Apache Airflow** pobiera surowe dane (CSV z **Kaggle**, ok. 1GB+), przetwarza je w **Apache Spark**, a nastÄ™pnie Å‚aduje do **BigQuery**.  
Proces skÅ‚ada siÄ™ z czterech gÅ‚Ã³wnych **DAG-Ã³w**:

- **`download_netflix_dag`** â€” pobiera i rozpakowuje dataset z Kaggle, uploaduje plik CSV do **Raw Zone** w **GCS**.  
- **`netflix_etl_dag`** â€” uruchamia job **Spark** odpowiedzialny za czyszczenie danych (usuwanie `NULL`, duplikatÃ³w, normalizacja) i zapisuje dane w formacie **Parquet** do **Processed Zone** w **GCS**.  
- **`load_to_bq_dag`** â€” Å‚aduje przetworzone dane z **GCS** do tabeli w **BigQuery**.  
- **`master_netflix_pipeline`** â€” nadrzÄ™dny DAG, ktÃ³ry sekwencyjnie uruchamia powyÅ¼sze sub-DAG-i (**download â†’ ETL â†’ load**) zgodnie z harmonogramem codziennym o **12:00 (czas polski)**.

Projekt zostaÅ‚ zrealizowany **lokalnie w Dockerze** (Airflow + Spark), symulujÄ…c architekturÄ™ **Data Lakehouse** w **Google Cloud Platform (GCP)**.  
W Å›rodowisku produkcyjnym **Airflow** mÃ³gÅ‚by dziaÅ‚aÄ‡ w **Cloud Composer**, a **Spark** w **Dataproc Serverless** â€” pipeline wykorzystuje te same koncepcje orkiestracji, przetwarzania oraz warstw danych (**Raw â†’ Processed â†’ Warehouse**).

---

## ğŸ—ï¸ Architektura danych

Pipeline opiera siÄ™ na koncepcji **Medallion Architecture (Lakehouse)**, obejmujÄ…cej warstwy danych od surowych po analityczne:

- **Raw Zone** â€” (`GCS bucket: airflow-and-spark-pipeline-raw`)  
  Surowe pliki **CSV** pobrane z **Kaggle**.
- **Temp Zone** â€” (`GCS bucket: airflow-and-spark-pipeline-temp`)  
  Dane poÅ›rednie / staging (obecnie niewykorzystywana, lecz gotowa na przyszÅ‚e rozszerzenia â€” np. joiny miÄ™dzy ÅºrÃ³dÅ‚ami lub inkrementalne loady).
- **Processed Zone** â€” (`GCS bucket: airflow-and-spark-pipeline-processed`)  
  Oczyszczone dane w formacie **Parquet**, bÄ™dÄ…ce wynikiem joba **Spark**.
- **Warehouse** â€” (`BigQuery dataset: airflow-and-spark-pipeline.netflix_data.titles_clean`)  
  Dane gotowe do analizy, z jasno zdefiniowanym schematem (`STRING` / `INTEGER` dla kolumn takich jak `show_id`, `title`, `release_year`).

---

### ğŸ“Š Schemat przepÅ‚ywu danych

```text
Kaggle API â†’ [download_netflix_dag] â†’ GCS Raw (CSV)
                          â†“
GCS Raw â†’ [netflix_etl_dag + Spark] â†’ GCS Processed (Parquet)
                          â†“
GCS Processed â†’ [load_to_bq_dag] â†’ BigQuery Warehouse
```

---

## ğŸ“ Struktura projektu

```text
airflow-and-spark-pipeline/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yaml              # GitHub Actions CI/CD
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ download_netflix_dag.py  # Pobieranie z Kaggle â†’ GCS
â”‚   â”‚   â”œâ”€â”€ netflix_etl_dag.py       # Trigger Spark ETL
â”‚   â”‚   â”œâ”€â”€ load_to_bq_dag.py        # Load do BigQuery
â”‚   â”‚   â””â”€â”€ master_netflix_pipeline.py # Master orchestrator
â”‚   â”œâ”€â”€ logs/                       # Logi Airflow
â”‚   â”œâ”€â”€ temp/                       # Tymczasowe dane (montowane w Docker)
â”‚   â”‚   â””â”€â”€ netflix/
â”‚   â”‚       â”œâ”€â”€ netflix_titles.csv
â”‚   â”‚       â””â”€â”€ netflix-shows.zip
â”‚   â”œâ”€â”€ gcp-key.json               # Klucz GCP (zignorowany w .gitignore)
â”‚   â”œâ”€â”€ kaggle-key.json            # Klucz Kaggle (zignorowany w .gitignore)
â”‚   â”œâ”€â”€ gcs-connector.jar          # JAR dla GCS w Spark
â”‚   â”œâ”€â”€ requirements.txt           # Deps Python: kaggle, pyspark, providers
â”‚   â”œâ”€â”€ docker-compose.yaml        # Kontenery: postgres, redis, scheduler, webserver, worker, init
â”‚   â””â”€â”€ Dockerfile                 # Custom obraz Airflow + Java
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ apps/
â”‚   â”‚   â””â”€â”€ netflix_etl.py         # PySpark job: clean + dedup + Parquet
â”‚   â”œâ”€â”€ data/                      # Dane testowe (opcjonalne)
â”‚   â”œâ”€â”€ docker-compose.yaml        # Spark master + worker (wspÃ³lna sieÄ‡ z Airflow)
â”‚   â””â”€â”€ Dockerfile                 # Custom Spark + GCS connector + delta-spark
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_dags.py               # Testy importu i zaleÅ¼noÅ›ci DAG-Ã³w
â”‚   â””â”€â”€ test_netflix_etl.py        # Testy Spark ETL (pytest, z mock danymi)
â”œâ”€â”€ start-local.sh                 # Skrypt: down + (opcjonalny) build + up Airflow + Spark
â”œâ”€â”€ .gitignore                     # Ignoruje logi, klucze, temp, dane
â”œâ”€â”€ Makefile                       # make test (uruchamia pytest)
â””â”€â”€ README.md                      

```

---

## âœ¨ Jak to dziaÅ‚a

**Master DAG** uruchamia siÄ™ codziennie o **12:00 (Europe/Warsaw)** i triggeruje sub-DAG-i sekwencyjnie (`wait_for_completion=True`).

### 1. Download
- **PythonOperator** pobiera plik ZIP z **Kaggle**, rozpakowuje CSV i uploaduje do **GCS Raw**.  
- ÅšcieÅ¼ki plikÃ³w przekazywane sÄ… miÄ™dzy taskami za pomocÄ… **XCom**.

### 2. ETL
- **SparkSubmitOperator** uruchamia `netflix_etl.py`.  
- Proces ETL:
  - Czyta CSV z GCS
  - Czyszczenie danych: `dropna()`, `trim()`, `lower()`, usuwa kolumnÄ™ `rating`
  - Usuwa duplikaty po `show_id`
  - Zapisuje wynik w formacie **Parquet** do **Processed Zone** w GCS  
- Konfiguracja poÅ‚Ä…czenia z GCS odbywa siÄ™ przez **conf / zmienne Å›rodowiskowe**.

### 3. Load
- **GCSToBigQueryOperator** Å‚aduje Parquet do **BigQuery**:
  - Tryb: **overwrite**
  - Schemat tabeli zdefiniowany w DAG-u

### Monitoring
- **Airflow UI**: DAG runs, logi, status taskÃ³w  
- **Spark UI**: statystyki executorÃ³w  
- **Retries** i `email_on_failure` skonfigurowane w `default_args` DAG-a

---

## ğŸ§© Wymagania

### Åšrodowisko:
**WSL2 + Docker Desktop** (lub Linux/Mac)

### Java 17:  
**Dla `SparkSubmitOperator` i testÃ³w:**

```bash
sudo apt install openjdk-17-jre-headless -y
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
echo 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64' >> ~/.bashrc
source ~/.bashrc
```

### Python dependencies (dla testÃ³w)
```bash
pip install apache-airflow==2.8.1 pytest
pip install -r airflow/requirements.txt  # kaggle, pyspark, providers
```

---

## â˜ï¸ GCP Setup

### 1. WÅ‚Ä…cz wymagane API
```bash
gcloud services enable storage.googleapis.com \
bigquery.googleapis.com \
iam.googleapis.com
```
### 2. UtwÃ³rz buckety w Cloud Storage
```bash
gsutil mb -l europe-central2 gs://airflow-and-spark-pipeline-raw/
gsutil mb -l europe-central2 gs://airflow-and-spark-pipeline-processed/
gsutil mb -l europe-central2 gs://airflow-and-spark-pipeline-temp/
```
### 3. UtwÃ³rz Service Account i przypisz role
```bash
gcloud iam service-accounts create airflow-spark-sa \
--display-name "Airflow and Spark Service Account"

gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> \
--member="serviceAccount:airflow-spark-sa@<YOUR_PROJECT_ID>.iam.gserviceaccount.com" \
--role="roles/storage.objectAdmin"

gcloud projects add-iam-policy-binding <YOUR_PROJECT_ID> \
--member="serviceAccount:airflow-spark-sa@<YOUR_PROJECT_ID>.iam.gserviceaccount.com" \
--role="roles/bigquery.jobUser"
```
ğŸ’¡ Alternatywnie: moÅ¼esz uÅ¼yÄ‡ uproszczonej roli:
```bash
--role="roles/infrastructure.admin"
```
### 4. Pobierz klucz i zapisz w katalogu airflow/
```bash
gcloud iam service-accounts keys create airflow/gcp-key.json \
--iam-account=airflow-spark-sa@<YOUR_PROJECT_ID>.iam.gserviceaccount.com
```
### 5. UtwÃ³rz dataset w BigQuery
```bash
bq --location=europe-central2 mk \
--dataset <YOUR_PROJECT_ID>:airflow-and-spark-pipeline
```
âœ… Po wykonaniu powyÅ¼szych krokÃ³w:

- Masz skonfigurowane API, bucketâ€™y i dataset.

- Plik gcp-key.json znajduje siÄ™ w katalogu airflow/ i bÄ™dzie uÅ¼ywany przez Airflow i Spark.

---

## ğŸ§  Kaggle Setup

### 1. Pobierz klucz API z Kaggle
1. WejdÅº na [https://www.kaggle.com/settings](https://www.kaggle.com/settings)  
2. PrzewiÅ„ do sekcji **API**  
3. Kliknij **"Create New API Token"**  
   â†’ Plik `kaggle.json` zostanie pobrany automatycznie.

### 2. Zapisz klucz w katalogu `airflow/`
ZmieÅ„ nazwÄ™ pobranego pliku i przenieÅ› go:
```bash
mv ~/Downloads/kaggle.json airflow/kaggle-key.json
```

---

## ğŸ³ Uruchomienie
### 1. StwÃ³rz sieÄ‡ Docker
```bash
docker network create airflow-spark-network
```

### 2. Uruchom kontenery
```bash
chmod +x start-local.sh
./start-local.sh --build  # Pierwszy raz: buduje obrazy
# Lub bez build, jeÅ›li obrazy juÅ¼ istniejÄ…:
./start-local.sh
```
**UI**
- Airflow UI: http://localhost:8080
- Spark UI: http://localhost:8081

---

## âš™ï¸ Konfiguracja poÅ‚Ä…czeÅ„ w Airflow UI

W panelu **Admin â†’ Connections** naleÅ¼y dodaÄ‡ poniÅ¼sze poÅ‚Ä…czenia:

| Conn ID               | Conn Type      | Host / Path                   | Additional Info                                                                 |
|------------------------|----------------|--------------------------------|----------------------------------------------------------------------------------|
| `spark_local`          | Spark          | `spark://spark-master:7077`    | â€”                                                                                |
| `google_cloud_default` | Google Cloud   | Keyfile Path: `/opt/airflow/gcp/gcp-key.json`<br>Project ID: `airflow-and-spark-pipeline` | 

---

## ğŸš¦ Uruchom pipeline

- W UI wyzwÃ³l rÄ™cznie DAG: master_netflix_pipeline
(lub poczekaj na schedule)

- SprawdÅº dane w bucketach GCS i tabelach BigQuery.

---

## ğŸ§ª Testy

Uruchom wszystkie testy:
```bash
make test
```

Lub rÄ™cznie:
```bash
pytest tests/test_netflix_etl.py -v
pytest tests/test_dags.py -v
```

**Pliki testÃ³w**

- test_dags.py â€“ sprawdza import DAG-Ã³w i zaleÅ¼noÅ›ci (np. master â†’ sub-DAG-i).

- test_netflix_etl.py â€“ symuluje dziaÅ‚anie Sparka (dropna, dedup, trim) z mock danymi.

---

## âœ… Gotowe!

**Po poprawnej konfiguracji:**

- Airflow automatycznie orchestruje taski Spark i GCP.

- Pipeline zapisuje dane z Kaggle do GCS i BigQuery.

- MoÅ¼esz rozwijaÄ‡ DAG-i lokalnie lub w Å›rodowisku produkcyjnym GCP.

---

## ğŸ”„ CI/CD

**GitHub Actions (ci.yaml): Automatycznie na push/PR do main:**

- Testy Python (pytest via Makefile).
- Linting: Black (format check), Flake8 (styl, ignore E501).
- Build Docker (Airflow + Spark) â€“ weryfikacja bez run.

## ğŸŒŸ Rozszerzenia i przemyÅ›lenia

- **Delta Lake**: Obecny Dockerfile Spark instaluje `delta-spark==3.2.0` â€“ Å‚atwo ulepszyÄ‡ ETL o ACID transactions i time travel (np. `df.write.format("delta").save()` zamiast Parquet), dodajÄ…c wersjonowanie do Processed Zone.  
- **Temp Zone**: Struktura bucketa gotowa na staging (np. dane poÅ›rednie przed joinami lub inkrementalnymi loadami do BigQuery).  
- **Produkcja**: Lokalny setup symuluje chmurÄ™; migracja: Airflow do Cloud Composer, Spark do Dataproc (z autoscaling). WspÃ³lna sieÄ‡ Docker pokazuje Å›wiadomoÅ›Ä‡ konteneryzacji (Kubernetes w GCP).

---

## ğŸ‘¤ Autor
Projekt przygotowany w celach edukacyjnych i demonstracyjnych.
MoÅ¼esz mnie znaleÅºÄ‡ na GitHubie: [tomsongracz](https://github.com/tomsongracz)
  










